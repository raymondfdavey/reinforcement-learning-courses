{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib_inline/config.py:66: DeprecationWarning: InlineBackend._figure_formats_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n",
      "  def _figure_formats_changed(self, name, old, new):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1147b51b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAFlCAYAAAA59ZWsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMrklEQVR4nO3d348d51nA8ec5u+u43jTY2E2K6ghXDUWKSkmNY6GmEohWVYGIChQuQESJuAWplYoIUOUvQAIuQEKIGxCVqiaAUJCAtpLVm6iopiQoSdMqjU3cEuqaNqnt/HB2z8vFGrTrPZuciefZObP5fK6ynrXzaOTxd9935pyTrbUAgAqToQcAYO8SGQDKiAwAZUQGgDIiA0AZkQGgzHKXbz5y5Eg7duxY0SgAjNHZs2fjwoULOetYp8gcO3YsTp8+3c9UAOwJJ06c2PGY7TIAyogMAGVEBoAyIgNAGZEBoIzIAFBGZAAoIzIAlBEZAMqIDABlRAaAMiIDQBmRAaCMyABQRmQAKCMyAJQRGQDKiAwAZUQGgDIiA0AZkQGgjMgAUEZkACgjMgCUERkAyogMAGVEBoAyIgNAGZEBoIzIAFBGZAAoIzIAlBEZAMqIDABlRAaAMiIDQBmRAaCMyABQRmQAKCMyAJQRGQDKiAwAZUQGgDIiA0AZkQGgjMgAUEZkACgjMgCUERkAyogMAGVEBoAyIgNAGZEBoIzIAFBGZAAoIzIAlBEZAMqIDABlRAaAMiIDQBmRAaCMyABQRmQAKCMyAJQRGQDKiAwAZUQGgDIiA0AZkQGgjMgAUEZkACgjMgCUERkAyogMAGVEBoAyIgNAGZEBoIzIAFBGZAAoIzIAlBEZAMqIDABlRAaAMiIDQBmRAaCMyABQRmQAKCMyAJQRGQDKiAwAZUQGgDIiA0AZkQGgjMgAUEZkACgjMgCUERkAyogMAGVEBoAyIgNAGZEBoIzIAFBGZAAoIzIAlBEZAMqIDABlRAaAMiIDQBmRAaCMyABQRmQAKCMyAJQRGQDKiAwAZUQGgDIiA0AZkQGgjMgAUEZkACgjMgCUERkAyogMAGVEBoAyIgNAGZEBoIzIAFBGZAAoIzIAlBEZAMqIDABlRAaAMiIDQBmRAaCMyABQRmQAKCMyAJQRGQDKLA89AIzFlcvfjxfPPTnz2GT5hvjh95yIzNzlqWCxiQzM6eXv/Vec/dJfzzy2cuCH4tC7PxC55JKCzWyXQU9amw49AiwckYE+tBYxFRm4lshAD1pEtOn60GPAwhEZ6EWL1kQGriUy0IcW0WyXwTYiA71oIgMziAz0xHYZbCcy0BcrGdhGZKAHrTVPl8EMIgM98WJM2E5koBdWMjCLyEAfPMIMM4kM9MKLMWEWkYGeWMnAdiIDPWjNSgZmERmYUy6txGRpZfbBNo31Vy7v7kAwAiIDc9p346HYf+hHZh6brl2JS9/55i5PBItPZGBOmRmRLhnowhUDc5tshAaYm8jAnDIz0koGOnHFwLwyIyYuGejCFQNzspKB7lwxMK90Twa6EhmYl6fLoDNXDMwpQ2SgK1cMzMs9GejMFQNzSvdkoDORgXl5hBk6c8XAnDzCDN25YmBembbLoCORgbl5ugy6csXAnDZu/LtkoAtXDMxrjhdjttZ2aRgYB5GBDl7vlozAwHYiA3N6w5v+bRoRQgObiQz0pE2nEVYzsIXIQE9am9oyg2uIDPTFdhlsIzLQE9tlsJ3IQE9sl8F2IgN9sV0G24gM9KQ122VwLZGBnrSp7TK4lshAX2yXwTYiAz2xkoHtRAZ60lpzTwauITLQk9bWw3YZbCUy0BcvxoRtRAZ60lpzTwauITLQk+bpMthGZKCDm46+L3Jpeeaxy+fPxJXLL+zuQLDgRAY6mKzsi4gdPrzMK/5hG5GBDiY52SkxwAwiA11MliLe6GOYgf8nMtBBTlwy0IUrBjrIydLQI8CoiAx0sBEZ22UwL5GBDjKtZKALkYEOcjKxkIEORAY6cE8GuhEZ6CBzEpYyMD+RgQ6sZKAbkYEOcrJkHQMdiAx0MZl4xT90IDLQwcR2GXQiMtCF18lAJyIDHWy8d5ntMpiXyEAHG48w78xHMMNWIgN9mq4PPQEsFJGBHk1FBrYQGehRayIDm4kM9MlKBrYQGejRdF1kYDORgR41KxnYQmSgR+7JwFYiAz1q0+nQI8BCERnoke0y2EpkoEciA1uJDPRIZGArkYEeiQxsJTLQJ5GBLUQGeuS9y2ArkYEe2S6DrUQGeiQysJXIQBeZceDw0R0Pv3ThuV0cBhafyEAHmRn7D96y4/FXXzy/i9PA4hMZ6CgnS0OPAKMhMtBJRqTIwLxEBjqykoH5iQx0NFkSGZjX8tADwJAeeuiheOCBB+b+/klm/MoHj8Wvfug9M48/d+65+OXbbovW2tx/5uHDh+PRRx+NlZWVuX8PjIXI8JZ28eLFOHPmzNzfnxlx4cdviojZkXnttbV49tlnO81w6dKlTlGCMREZ6Gh9fSMIV6Y3xAtrt8TadCXetnQxDi57fBmuJTLQ0dp0GuttEk9d/mBcuHI01toNcWDpxfixA6cj4vGhx4OF4sY/dNEi1tan8cxLPxXPv3pbvNbeFi0mcXn9UHzt0l1xaf3Q0BPCQhEZ6KBFxPMvH40zL/9kROSWY6+2A/HYDz4yyFywqEQGOnptPaLtcOmsh8ebYTORgY7W1qdDjwCjITLQ0fpUZGBeIgMdHZyci1v2nYmNOzSbTeP21UeHGAkWlshAR216Jd7/9lNx49L3YxJrEdFiOV+J9x74Srxjn8+Tgc28TgY6ev5/LsY/f/mpeGn9XHzrlffGlXYgDi5/J1654RvxwqWXhx4PFkp2eTuLm2++ud1zzz2F48Duevrpp+PUqVODzrB///647777YjKxscA4Pfzww3H+/PmcdazTSubw4cNx//339zIULIJHHnlkISJz7733eoNMRuv1rqFOkVldXY2TJ09e90CwKJ544omhR4iVlZW48847Y9++fUOPAm/K6urqjseszwEoIzIAlBEZAMqIDABlRAaAMiIDQBmRAaCMyABQRmQAKCMyAJTxLsy8pd1xxx3x4IMPDjrD6upqLC352Gb2JpHhLe348eNx/PjxoceAPct2GQBlRAaAMiIDQBmRAaCMyABQRmQAKCMyAJQRGQDKiAwAZUQGgDIiA0AZkQGgjMgAUEZkACgjMgCUERkAyogMAGVEBoAyIgNAGZEBoIzIAFBGZAAoIzIAlBEZAMqIDABlRAaAMiIDQBmRAaCMyABQRmQAKCMyAJQRGQDKiAwAZUQGgDIiA0AZkQGgjMgAUEZkACgjMgCUERkAyogMAGVEBoAyIgNAGZEBoIzIAFBGZAAoIzIAlBEZAMqIDABlRAaAMiIDQBmRAaCMyABQRmQAKCMyAJQRGQDKiAwAZUQGgDIiA0AZkQGgjMgAUEZkACgjMgCUERkAyogMAGVEBoAyIgNAGZEBoEy21ub/5szvRsR/1o3zho5ExIUB//97hfN4/ZzDfjiP128RzuGPttbeMetAp8gMLTNPt9ZODD3H2DmP18857IfzeP0W/RzaLgOgjMgAUGZskfmLoQfYI5zH6+cc9sN5vH4LfQ5HdU8GgHEZ20oGgBEZbWQy81OZ2TLzyNCzjFFm/mFmPp2Z/5GZf5+ZB4eeaSwy82OZ+fXMfCYzf2/oecYmM2/NzFOZ+VRmPpmZnxh6pjHLzKXM/PfM/MehZ5lllJHJzFsj4qMR8dzQs4zYFyLifa2190fENyLi9weeZxQycyki/iwifj4ibo+IX8vM24edanTWIuJTrbXbI+KnI+K3nMPr8omI+NrQQ+xklJGJiD+OiN+NCDeU3qTW2udba2tXv/xyRBwdcp4RORkRz7TWnm2tXYmIz0bExweeaVRaa8+31r569b8vxsY/kO8adqpxysyjEfGLEfGXQ8+yk9FFJjM/HhHfbq09PvQse8hvRsQ/DT3ESLwrIs5t+vpb4R/INy0zj0XEByLiXwceZaz+JDZ+4J4OPMeOloceYJbM/GJEvHPGoU9HxB/ExlYZb+D1zmNr7R+ufs+nY2P74jO7ORtk5o0R8bcR8cnW2g+GnmdsMvPuiDjfWvu3zPzZgcfZ0UJGprX2kVm/npk/ERHvjojHMzNiY4vnq5l5srX237s44ijsdB7/T2beHxF3R8SHm2fZ5/XtiLh109dHr/4aHWTmSmwE5jOttb8bep6RuisifikzfyEi9kfETZn5N6213xh4ri1G/TqZzDwbESdaa0O/OdzoZObHIuKPIuJnWmvfHXqescjM5dh4UOLDsRGXr0TEr7fWnhx0sBHJjZ8Q/yoivtda++TA4+wJV1cyv9Nau3vgUbYZ3T0ZevOnEfH2iPhCZj6WmX8+9EBjcPVhid+OiH+JjRvWnxOYzu6KiHsj4ueu/t177OpP4+xBo17JALDYrGQAKCMyAJQRGQDKiAwAZUQGgDIiA0AZkQGgjMgAUOZ/AZWvZEINjyr3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SHOW A FRAME\n",
    "env.reset()\n",
    "frame = env.render(mode='rgb_array')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.axes.yaxis.set_visible(False)\n",
    "min_x = env.observation_space.low[0]\n",
    "max_x = env.observation_space.high[0]\n",
    "ax.imshow(frame, extent=[min_x, max_x, 0, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cart position from -4.80 to 4.80\n",
      "Cart velocity from -3.40E+38 to 3.40E+38\n",
      "Angle from -0.42 to 0.42\n",
      "Angular velocity from -3.40E+38 to 3.40E+38\n"
     ]
    }
   ],
   "source": [
    "# EXPLORE STATE SPACE\n",
    "x_min, v_min, angle_min, angular_v_min = env.observation_space.low\n",
    "x_max, v_max, angle_max, angular_v_max = env.observation_space.high\n",
    "\n",
    "print(f'Cart position from {x_min:.2f} to {x_max:.2f}')\n",
    "print(f'Cart velocity from {v_min:.2E} to {v_max:.2E}')\n",
    "print(f'Angle from {angle_min:.2f} to {angle_max:.2f}')\n",
    "print(f'Angular velocity from {angular_v_min:.2E} to {angular_v_max:.2E}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENVIRONMENT, ACTIONS, STATES, REWARDS\n",
    "\n",
    "![](./reinforcement_learning_course/pau_bajo_course/cartpole_image.jpeg)\n",
    "\n",
    "### <ins>State</ins>\n",
    "\n",
    "State is represented by 4 values shown above:\n",
    "\n",
    " - the cart position **x** from `-2.4` to `2.4`\n",
    " - the cart velocity **v**\n",
    " - the pole angle $\\theta$ with respect to the vertical. From `-12` to `12` degrees (`-0.21` to `0.21` in radians)\n",
    " - the pole angular velocity $\\omega$. This is the rate of change of $\\theta$\n",
    "\n",
    " ### <ins>Episode starts and ends</ins>\n",
    "\n",
    "An episode terminates when either:\n",
    "\n",
    " - the cart goes beyond the limits: `x > 2.4` or x `< -2.4`\n",
    " - the pole is too far from the vertical: `θ > 12` degrees or `θ < -12` degrees.\n",
    " - or we reached the maximum number of episodes steps, `500`. In this case, the agent perfectly solved the episode.\n",
    "\n",
    "The starting state is sampled randomly from the interval `[-0.05, 0.05]` for each state. Sometimes, the starting position is so close to balance that the episode is easy. Other times, the starting position is so off-balance that the episode is much harder to solve, and sometimes even impossible.\n",
    "\n",
    "### <ins>Actions</ins>\n",
    "\n",
    "What about the **actions** our agent can perform?\n",
    " - 0 : Push the cart to the left.\n",
    " - 1 : Push the cart to the right.\n",
    "\n",
    "### <ins>Rewards</ins>\n",
    "The reward is `+1` for every step taken. This means that the longer the agent keeps the pole standing, the higher the cumulative reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_state: [-0.03781126  0.04057393  0.0331523  -0.02027966]\n",
      "random_action: 0\n",
      "next_state: [-0.03699978 -0.15500739  0.0327467   0.2826761 ]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "# SEE INTERACTION BETWEEN TWO STATES AND AN ACTION\n",
    "# OLD STATE PLUS A STEP ACCORDING TO AN ACTION GENERATED NEW STATE AND REWARD\n",
    "\n",
    "old_state = env.reset()\n",
    "env.state = old_state\n",
    "print(f\"old_state: {old_state}\")  # old_state: [-0.01298927 -0.04985128 -0.0408152  -0.04901766]\n",
    "\n",
    "random_action = env.action_space.sample()\n",
    "print(f\"random_action: {random_action}\") # random_action: 1\n",
    "\n",
    "next_state, reward, done, info = env.step(random_action)\n",
    "\n",
    "print(f\"next_state: {next_state}\") # next_state: [-0.01554931 -0.18837169  0.0489684   0.35493335]\n",
    "# here next_state is a list [x, v, theta, omega]\n",
    "print(f\"reward: {reward}\") # reward: 1.0\n",
    "print(f\"done: {done}\") # done: False\n",
    "print(f\"info: {info}\") # info: {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE RANDOM PLAYER\n",
    "episodes = 10\n",
    "\n",
    "\n",
    "\n",
    "def Random_games(episodes):\n",
    "    '''\n",
    "    this function takes a number of episodes, randomly initialises the cartpole environment and then picks random actions up to a max of 500 goes to see how it does,\n",
    "    '''\n",
    "    # Each of this episode is its own game.\n",
    "    max_t = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        #initialises the environment radomly\n",
    "        env.reset()\n",
    "        \n",
    "        # this is each frame, up to 500 (which is max value of steps)...but we wont make it that far with random.\n",
    "        for t in range(500):\n",
    "\n",
    "            # This will display the environment\n",
    "            # Only display if you really want to see it.\n",
    "            # Takes much longer to display it.\n",
    "            env.render()\n",
    "            \n",
    "            # This will just create a sample action in any environment. it just picks randomly between the poss actions, in this case 0 or 1\n",
    "            # In this environment, the action can be 0 or 1, which is left or right\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # this executes the environment with an action, \n",
    "            # and returns the observation of the environment, \n",
    "            # the reward, if the env is over, and other info.\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # lets print everything in one line:\n",
    "            print(t, next_state, reward, done, info, action)\n",
    "\n",
    "            if done:\n",
    "                if t > max_t:\n",
    "                    max_t = t\n",
    "                break\n",
    "    print(f\"max_t: {max_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.03391917 -0.18016492 -0.0390009   0.27300513] 1.0 False {} 0\n",
      "1 [ 0.03031587  0.0154912  -0.03354079 -0.03171922] 1.0 False {} 1\n",
      "2 [ 0.03062569  0.21107769 -0.03417518 -0.3347931 ] 1.0 False {} 1\n",
      "3 [ 0.03484724  0.01645836 -0.04087104 -0.05308012] 1.0 False {} 0\n",
      "4 [ 0.03517641  0.2121418  -0.04193264 -0.3583729 ] 1.0 False {} 1\n",
      "5 [ 0.03941925  0.01764027 -0.0491001  -0.07920165] 1.0 False {} 0\n",
      "6 [ 0.03977205 -0.17674468 -0.05068414  0.19759475] 1.0 False {} 0\n",
      "7 [ 0.03623716  0.01906421 -0.04673224 -0.11063616] 1.0 False {} 1\n",
      "8 [ 0.03661845 -0.17535803 -0.04894496  0.16694452] 1.0 False {} 0\n",
      "9 [ 0.03311128  0.02042913 -0.04560607 -0.14076857] 1.0 False {} 1\n",
      "10 [ 0.03351986 -0.17401098 -0.04842144  0.13718478] 1.0 False {} 0\n",
      "11 [ 0.03003965 -0.36840716 -0.04567775  0.41420692] 1.0 False {} 0\n",
      "12 [ 0.0226715  -0.5628529  -0.03739361  0.6921467 ] 1.0 False {} 0\n",
      "13 [ 0.01141445 -0.75743663 -0.02355067  0.97282714] 1.0 False {} 0\n",
      "14 [-0.00373429 -0.9522348  -0.00409413  1.2580202 ] 1.0 False {} 0\n",
      "15 [-0.02277898 -1.1473042   0.02106627  1.549418  ] 1.0 False {} 0\n",
      "16 [-0.04572507 -0.9524412   0.05205463  1.2633815 ] 1.0 False {} 1\n",
      "17 [-0.06477389 -1.1481886   0.07732226  1.5719019 ] 1.0 False {} 0\n",
      "18 [-0.08773766 -0.9540696   0.1087603   1.3043036 ] 1.0 False {} 1\n",
      "19 [-0.10681906 -0.76048195  0.13484637  1.0475503 ] 1.0 False {} 1\n",
      "20 [-0.12202869 -0.95711046  0.15579738  1.3793404 ] 1.0 False {} 0\n",
      "21 [-0.1411709  -0.76423866  0.1833842   1.1391562 ] 1.0 False {} 1\n",
      "22 [-0.15645568 -0.5719252   0.20616731  0.9091363 ] 1.0 False {} 1\n",
      "23 [-0.16789418 -0.38010004  0.22435004  0.6876768 ] 1.0 True {} 1\n",
      "0 [ 0.04603698  0.23511861 -0.02436923 -0.29941723] 1.0 False {} 1\n",
      "1 [ 0.05073935  0.04035234 -0.03035757 -0.0145184 ] 1.0 False {} 0\n",
      "2 [ 0.0515464   0.2358962  -0.03064794 -0.31662282] 1.0 False {} 1\n",
      "3 [ 0.05626432  0.43144098 -0.0369804  -0.6188113 ] 1.0 False {} 1\n",
      "4 [ 0.06489314  0.6270594  -0.04935662 -0.9229081 ] 1.0 False {} 1\n",
      "5 [ 0.07743433  0.8228122  -0.06781479 -1.2306846 ] 1.0 False {} 1\n",
      "6 [ 0.09389058  1.0187378  -0.09242848 -1.5438198 ] 1.0 False {} 1\n",
      "7 [ 0.11426533  0.8248401  -0.12330487 -1.28135   ] 1.0 False {} 0\n",
      "8 [ 0.13076213  1.021298   -0.14893188 -1.6099595 ] 1.0 False {} 1\n",
      "9 [ 0.15118809  1.2178327  -0.18113106 -1.9451222 ] 1.0 False {} 1\n",
      "10 [ 0.17554475  1.4143642  -0.22003351 -2.2880588 ] 1.0 True {} 1\n",
      "0 [ 0.02895549  0.15866053 -0.00295718 -0.29514518] 1.0 False {} 1\n",
      "1 [ 0.0321287   0.35382453 -0.00886008 -0.5887593 ] 1.0 False {} 1\n",
      "2 [ 0.03920519  0.5490694  -0.02063527 -0.88421994] 1.0 False {} 1\n",
      "3 [ 0.05018658  0.7444654  -0.03831967 -1.1833179 ] 1.0 False {} 1\n",
      "4 [ 0.06507589  0.940063   -0.06198602 -1.4877622 ] 1.0 False {} 1\n",
      "5 [ 0.08387715  0.7457486  -0.09174126 -1.2150625 ] 1.0 False {} 0\n",
      "6 [ 0.09879212  0.551922   -0.11604252 -0.9524794 ] 1.0 False {} 0\n",
      "7 [ 0.10983056  0.74839807 -0.13509211 -1.2792515 ] 1.0 False {} 1\n",
      "8 [ 0.12479852  0.94495803 -0.16067713 -1.6110015 ] 1.0 False {} 1\n",
      "9 [ 0.14369768  0.7520572  -0.19289716 -1.3724133 ] 1.0 False {} 0\n",
      "10 [ 0.15873882  0.55979824 -0.22034542 -1.1457344 ] 1.0 True {} 0\n",
      "0 [ 0.01879944 -0.15667112  0.04755935  0.281082  ] 1.0 False {} 0\n",
      "1 [0.01566602 0.0377413  0.05318099 0.00377029] 1.0 False {} 1\n",
      "2 [ 0.01642085  0.23206182  0.05325639 -0.27167055] 1.0 False {} 1\n",
      "3 [0.02106208 0.03622197 0.04782298 0.03732269] 1.0 False {} 0\n",
      "4 [ 0.02178652 -0.15955198  0.04856944  0.34470248] 1.0 False {} 0\n",
      "5 [0.01859548 0.0348466  0.05546349 0.06772227] 1.0 False {} 1\n",
      "6 [ 0.01929241  0.22913133  0.05681793 -0.20695877] 1.0 False {} 1\n",
      "7 [ 0.02387504  0.4233967   0.05267876 -0.48119113] 1.0 False {} 1\n",
      "8 [ 0.03234297  0.22757228  0.04305493 -0.17238158] 1.0 False {} 0\n",
      "9 [ 0.03689442  0.4220524   0.0396073  -0.45117718] 1.0 False {} 1\n",
      "10 [ 0.04533547  0.61659247  0.03058376 -0.7311169 ] 1.0 False {} 1\n",
      "11 [ 0.05766732  0.8112787   0.01596142 -1.0140195 ] 1.0 False {} 1\n",
      "12 [ 0.07389289  1.0061842  -0.00431897 -1.301648  ] 1.0 False {} 1\n",
      "13 [ 0.09401657  1.2013607  -0.03035193 -1.5956798 ] 1.0 False {} 1\n",
      "14 [ 0.11804379  1.396829   -0.06226553 -1.8976694 ] 1.0 False {} 1\n",
      "15 [ 0.14598037  1.5925676  -0.10021891 -2.2090025 ] 1.0 False {} 1\n",
      "16 [ 0.17783172  1.3985392  -0.14439896 -1.9488353 ] 1.0 False {} 0\n",
      "17 [ 0.2058025   1.5948727  -0.18337567 -2.2825766 ] 1.0 False {} 1\n",
      "18 [ 0.23769996  1.4018648  -0.2290272  -2.0515294 ] 1.0 True {} 0\n",
      "0 [ 0.0462695   0.23507486 -0.01892245 -0.29087496] 1.0 False {} 1\n",
      "1 [ 0.05097099  0.04022777 -0.02473995 -0.00421949] 1.0 False {} 0\n",
      "2 [ 0.05177555 -0.1545308  -0.02482434  0.28055617] 1.0 False {} 0\n",
      "3 [ 0.04868494 -0.34929    -0.01921321  0.5653074 ] 1.0 False {} 0\n",
      "4 [ 0.04169913 -0.15390384 -0.00790707  0.26663387] 1.0 False {} 1\n",
      "5 [ 0.03862106  0.04133006 -0.00257439 -0.02853249] 1.0 False {} 1\n",
      "6 [ 0.03944766 -0.15375488 -0.00314504  0.26333708] 1.0 False {} 0\n",
      "7 [ 0.03637256 -0.3488318   0.0021217   0.55502635] 1.0 False {} 0\n",
      "8 [ 0.02939593 -0.54398346  0.01322223  0.848377  ] 1.0 False {} 0\n",
      "9 [ 0.01851626 -0.34904432  0.03018977  0.5598811 ] 1.0 False {} 1\n",
      "10 [ 0.01153537 -0.15435883  0.04138739  0.27686036] 1.0 False {} 1\n",
      "11 [ 0.00844819  0.04014897  0.0469246  -0.00248708] 1.0 False {} 1\n",
      "12 [ 0.00925117 -0.15561344  0.04687486  0.30462408] 1.0 False {} 0\n",
      "13 [0.0061389  0.03881024 0.05296734 0.02708505] 1.0 False {} 1\n",
      "14 [ 0.00691511  0.23313417  0.05350904 -0.24842693] 1.0 False {} 1\n",
      "15 [ 0.01157779  0.42745274  0.0485405  -0.5237634 ] 1.0 False {} 1\n",
      "16 [ 0.02012685  0.62185913  0.03806523 -0.80076426] 1.0 False {} 1\n",
      "17 [ 0.03256403  0.8164389   0.02204995 -1.081234  ] 1.0 False {} 1\n",
      "18 [ 4.8892807e-02  1.0112629e+00  4.2526852e-04 -1.3669169e+00] 1.0 False {} 1\n",
      "19 [ 0.06911807  0.81613564 -0.02691307 -1.074101  ] 1.0 False {} 0\n",
      "20 [ 0.08544078  1.0116028  -0.04839509 -1.3751069 ] 1.0 False {} 1\n",
      "21 [ 0.10567284  0.8171178  -0.07589722 -1.0979437 ] 1.0 False {} 0\n",
      "22 [ 0.12201519  0.6230726  -0.0978561  -0.8300062 ] 1.0 False {} 0\n",
      "23 [ 0.13447665  0.81938624 -0.11445622 -1.1517917 ] 1.0 False {} 1\n",
      "24 [ 0.15086436  1.0158001  -0.13749206 -1.4780614 ] 1.0 False {} 1\n",
      "25 [ 0.17118037  1.212307   -0.16705328 -1.8103354 ] 1.0 False {} 1\n",
      "26 [ 0.19542651  1.0193951  -0.20325999 -1.5738814 ] 1.0 False {} 0\n",
      "27 [ 0.21581441  1.2162776  -0.23473762 -1.9224733 ] 1.0 True {} 1\n",
      "0 [ 0.04937326 -0.19880247  0.01775197  0.25077903] 1.0 False {} 0\n",
      "1 [ 0.04539721 -0.00393847  0.02276755 -0.03625212] 1.0 False {} 1\n",
      "2 [ 0.04531844 -0.19937938  0.02204251  0.26352635] 1.0 False {} 0\n",
      "3 [ 0.04133085 -0.00457888  0.02731303 -0.02212344] 1.0 False {} 1\n",
      "4 [ 0.04123928  0.19014095  0.02687056 -0.3060652 ] 1.0 False {} 1\n",
      "5 [ 0.04504209  0.38486987  0.02074926 -0.5901542 ] 1.0 False {} 1\n",
      "6 [ 0.05273949  0.18946365  0.00894618 -0.29100809] 1.0 False {} 0\n",
      "7 [ 0.05652877 -0.00578471  0.00312601  0.00448289] 1.0 False {} 0\n",
      "8 [ 0.05641307 -0.20095135  0.00321567  0.29815048] 1.0 False {} 0\n",
      "9 [ 0.05239404 -0.00587539  0.00917868  0.00648345] 1.0 False {} 1\n",
      "10 [ 0.05227654  0.18911374  0.00930835 -0.28328943] 1.0 False {} 1\n",
      "11 [ 0.05605881 -0.00613974  0.00364256  0.01231472] 1.0 False {} 0\n",
      "12 [ 0.05593602 -0.20131375  0.00388886  0.30614468] 1.0 False {} 0\n",
      "13 [ 0.05190974 -0.3964909   0.01001175  0.6000515 ] 1.0 False {} 0\n",
      "14 [ 0.04397992 -0.20151043  0.02201278  0.3105389 ] 1.0 False {} 1\n",
      "15 [ 0.03994972 -0.39693898  0.02822356  0.6100819 ] 1.0 False {} 0\n",
      "16 [ 0.03201094 -0.20222268  0.0404252   0.32642043] 1.0 False {} 1\n",
      "17 [ 0.02796648 -0.39789617  0.04695361  0.6315726 ] 1.0 False {} 0\n",
      "18 [ 0.02000856 -0.5936407   0.05958506  0.9386651 ] 1.0 False {} 0\n",
      "19 [ 0.00813574 -0.7895132   0.07835836  1.24946   ] 1.0 False {} 0\n",
      "20 [-0.00765452 -0.9855472   0.10334756  1.5656226 ] 1.0 False {} 0\n",
      "21 [-0.02736546 -0.79180104  0.13466     1.306884  ] 1.0 False {} 1\n",
      "22 [-0.04320148 -0.988348    0.1607977   1.638506  ] 1.0 False {} 0\n",
      "23 [-0.06296844 -1.1849483   0.19356781  1.9766735 ] 1.0 False {} 0\n",
      "24 [-0.08666741 -0.9923219   0.23310128  1.7496842 ] 1.0 True {} 1\n",
      "0 [ 0.04678395 -0.2431032  -0.00731094  0.3316971 ] 1.0 False {} 0\n",
      "1 [ 0.04192188 -0.43812034 -0.00067699  0.62206554] 1.0 False {} 0\n",
      "2 [ 0.03315948 -0.24298893  0.01176432  0.3291695 ] 1.0 False {} 1\n",
      "3 [ 0.02829969 -0.43827635  0.01834771  0.625539  ] 1.0 False {} 0\n",
      "4 [ 0.01953417 -0.6336496   0.03085849  0.9239434 ] 1.0 False {} 0\n",
      "5 [ 0.00686118 -0.82917446  0.04933736  1.2261621 ] 1.0 False {} 0\n",
      "6 [-0.00972231 -1.0248957   0.0738606   1.533886  ] 1.0 False {} 0\n",
      "7 [-0.03022023 -1.2208256   0.10453832  1.8486749 ] 1.0 False {} 0\n",
      "8 [-0.05463674 -1.0269985   0.14151181  1.5901997 ] 1.0 False {} 1\n",
      "9 [-0.07517671 -1.223489    0.17331581  1.923455  ] 1.0 False {} 0\n",
      "10 [-0.09964649 -1.0306004   0.21178491  1.689157  ] 1.0 True {} 1\n",
      "0 [ 0.00210071 -0.1970644  -0.01214904  0.24347572] 1.0 False {} 0\n",
      "1 [-0.00184058 -0.39201072 -0.00727953  0.5323019 ] 1.0 False {} 0\n",
      "2 [-0.00968079 -0.19678715  0.00336651  0.23733413] 1.0 False {} 1\n",
      "3 [-0.01361654 -0.00171346  0.00811319 -0.05428499] 1.0 False {} 1\n",
      "4 [-0.01365081  0.19329123  0.00702749 -0.34439713] 1.0 False {} 1\n",
      "5 [-0.00978498 -0.00192999  0.00013955 -0.04950649] 1.0 False {} 0\n",
      "6 [-0.00982358  0.19318996 -0.00085058 -0.34214538] 1.0 False {} 1\n",
      "7 [-0.00595978  0.388324   -0.00769349 -0.63509643] 1.0 False {} 1\n",
      "8 [ 0.0018067   0.5835524  -0.02039541 -0.93019223] 1.0 False {} 1\n",
      "9 [ 0.01347775  0.3887116  -0.03899926 -0.6439876 ] 1.0 False {} 0\n",
      "10 [ 0.02125198  0.19415425 -0.05187901 -0.36383638] 1.0 False {} 0\n",
      "11 [ 0.02513506 -0.00019347 -0.05915574 -0.08795259] 1.0 False {} 0\n",
      "12 [ 0.02513119 -0.1944198  -0.06091479  0.18549563] 1.0 False {} 0\n",
      "13 [ 0.0212428   0.00151843 -0.05720488 -0.12576446] 1.0 False {} 1\n",
      "14 [ 0.02127317 -0.19273932 -0.05972017  0.14833649] 1.0 False {} 0\n",
      "15 [ 0.01741838 -0.3869575  -0.05675344  0.4215971 ] 1.0 False {} 0\n",
      "16 [ 0.00967923 -0.19107932 -0.0483215   0.11157632] 1.0 False {} 1\n",
      "17 [ 0.00585764 -0.38547674 -0.04608997  0.3886311 ] 1.0 False {} 0\n",
      "18 [-0.00185189 -0.5799152  -0.03831735  0.66643345] 1.0 False {} 0\n",
      "19 [-0.0134502  -0.38428187 -0.02498868  0.3619363 ] 1.0 False {} 1\n",
      "20 [-0.02113583 -0.57903993 -0.01774995  0.6466362 ] 1.0 False {} 0\n",
      "21 [-0.03271663 -0.7739101  -0.00481723  0.93367726] 1.0 False {} 0\n",
      "22 [-0.04819483 -0.5787235   0.01385632  0.63948447] 1.0 False {} 1\n",
      "23 [-0.0597693  -0.7740359   0.02664601  0.9364985 ] 1.0 False {} 0\n",
      "24 [-0.07525002 -0.96950686  0.04537598  1.2374339 ] 1.0 False {} 0\n",
      "25 [-0.09464016 -1.1651814   0.07012466  1.5439796 ] 1.0 False {} 0\n",
      "26 [-0.11794379 -0.97096866  0.10100425  1.2739762 ] 1.0 False {} 1\n",
      "27 [-0.13736317 -1.1672237   0.12648377  1.5965031 ] 1.0 False {} 0\n",
      "28 [-0.16070764 -1.363598    0.15841384  1.9257987 ] 1.0 False {} 0\n",
      "29 [-0.1879796  -1.170491    0.19692981  1.6861442 ] 1.0 False {} 1\n",
      "30 [-0.21138941 -0.9781169   0.23065269  1.4606844 ] 1.0 True {} 1\n",
      "0 [-0.01201409  0.1812648   0.01656125 -0.29936936] 1.0 False {} 1\n",
      "1 [-0.00838879 -0.01408925  0.01057387 -0.00150968] 1.0 False {} 0\n",
      "2 [-0.00867058  0.18087947  0.01054367 -0.29083776] 1.0 False {} 1\n",
      "3 [-0.00505299 -0.01439122  0.00472692  0.00515178] 1.0 False {} 0\n",
      "4 [-0.00534082 -0.20958064  0.00482995  0.29932237] 1.0 False {} 0\n",
      "5 [-0.00953243 -0.01452787  0.0108164   0.00816661] 1.0 False {} 1\n",
      "6 [-0.00982299 -0.20980327  0.01097973  0.30424252] 1.0 False {} 0\n",
      "7 [-0.01401905 -0.40507996  0.01706458  0.60036784] 1.0 False {} 0\n",
      "8 [-0.02212065 -0.21020083  0.02907194  0.3131085 ] 1.0 False {} 1\n",
      "9 [-0.02632467 -0.01550484  0.03533411  0.02973386] 1.0 False {} 1\n",
      "10 [-0.02663476  0.17909305  0.03592879 -0.25159472] 1.0 False {} 1\n",
      "11 [-0.0230529   0.37368402  0.03089689 -0.532732  ] 1.0 False {} 1\n",
      "12 [-0.01557922  0.5683581   0.02024225 -0.81552154] 1.0 False {} 1\n",
      "13 [-0.00421206  0.37296492  0.00393182 -0.51654094] 1.0 False {} 0\n",
      "14 [ 0.00324724  0.5680313  -0.006399   -0.8079823 ] 1.0 False {} 1\n",
      "15 [ 0.01460786  0.3729976  -0.02255864 -0.5173191 ] 1.0 False {} 0\n",
      "16 [ 0.02206782  0.17820044 -0.03290503 -0.23182935] 1.0 False {} 0\n",
      "17 [ 0.02563182 -0.01643626 -0.03754161  0.05029533] 1.0 False {} 0\n",
      "18 [ 0.0253031   0.17920333 -0.03653571 -0.25399193] 1.0 False {} 1\n",
      "19 [ 0.02888717 -0.01537841 -0.04161555  0.0269468 ] 1.0 False {} 0\n",
      "20 [ 0.0285796  -0.20987962 -0.04107661  0.30621457] 1.0 False {} 0\n",
      "21 [ 0.02438201 -0.01419714 -0.03495232  0.00086532] 1.0 False {} 1\n",
      "22 [ 0.02409806 -0.20880084 -0.03493501  0.28231868] 1.0 False {} 0\n",
      "23 [ 0.01992205 -0.01319845 -0.02928864 -0.02117466] 1.0 False {} 1\n",
      "24 [ 0.01965808 -0.2078884  -0.02971213  0.26212537] 1.0 False {} 0\n",
      "25 [ 0.01550031 -0.4025739  -0.02446962  0.54529065] 1.0 False {} 0\n",
      "26 [ 0.00744883 -0.20711683 -0.01356381  0.24499945] 1.0 False {} 1\n",
      "27 [ 0.00330649 -0.01180379 -0.00866382 -0.05193077] 1.0 False {} 1\n",
      "28 [ 0.00307042  0.18344131 -0.00970243 -0.34733456] 1.0 False {} 1\n",
      "29 [ 0.00673924  0.3786999  -0.01664913 -0.64306116] 1.0 False {} 1\n",
      "30 [ 0.01431324  0.5740499  -0.02951035 -0.94094014] 1.0 False {} 1\n",
      "31 [ 0.02579424  0.37933785 -0.04832915 -0.657674  ] 1.0 False {} 0\n",
      "32 [ 0.033381    0.18492074 -0.06148263 -0.3805921 ] 1.0 False {} 0\n",
      "33 [ 0.03707941 -0.00927671 -0.06909447 -0.10791083] 1.0 False {} 0\n",
      "34 [ 0.03689388 -0.20334399 -0.07125269  0.16219789] 1.0 False {} 0\n",
      "35 [ 0.032827   -0.3973774  -0.06800874  0.43157893] 1.0 False {} 0\n",
      "36 [ 0.02487945 -0.59147376 -0.05937716  0.7020713 ] 1.0 False {} 0\n",
      "37 [ 0.01304998 -0.78572464 -0.04533573  0.9754875 ] 1.0 False {} 0\n",
      "38 [-0.00266452 -0.9802102  -0.02582598  1.2535919 ] 1.0 False {} 0\n",
      "39 [-2.2268720e-02 -1.1749920e+00 -7.5414311e-04  1.5380752e+00] 1.0 False {} 0\n",
      "40 [-0.04576856 -1.3701049   0.03000736  1.8305227 ] 1.0 False {} 0\n",
      "41 [-0.07317065 -1.5655459   0.06661782  2.132373  ] 1.0 False {} 0\n",
      "42 [-0.10448158 -1.3711443   0.10926528  1.8609886 ] 1.0 False {} 1\n",
      "43 [-0.13190447 -1.1773771   0.14648505  1.6041312 ] 1.0 False {} 1\n",
      "44 [-0.155452   -0.9842606   0.17856768  1.3604716 ] 1.0 False {} 1\n",
      "45 [-0.17513722 -1.1811142   0.20577711  1.7032772 ] 1.0 False {} 0\n",
      "46 [-0.1987595  -1.3779252   0.23984265  2.0523376 ] 1.0 True {} 0\n",
      "0 [ 0.00978949 -0.17009431 -0.02160571  0.26135948] 1.0 False {} 0\n",
      "1 [ 0.0063876   0.02532928 -0.01637852 -0.03805908] 1.0 False {} 1\n",
      "2 [ 0.00689418  0.22068222 -0.0171397  -0.33586428] 1.0 False {} 1\n",
      "3 [ 0.01130783  0.02580833 -0.02385698 -0.04863519] 1.0 False {} 0\n",
      "4 [ 0.011824   -0.16896355 -0.02482969  0.2364262 ] 1.0 False {} 0\n",
      "5 [ 0.00844472 -0.36372212 -0.02010116  0.52117485] 1.0 False {} 0\n",
      "6 [ 0.00117028 -0.16832307 -0.00967767  0.22222613] 1.0 False {} 1\n",
      "7 [-0.00219618  0.02693586 -0.00523314 -0.07349373] 1.0 False {} 1\n",
      "8 [-0.00165746  0.22213244 -0.00670302 -0.36782312] 1.0 False {} 1\n",
      "9 [ 0.00278519  0.41734898 -0.01405948 -0.6626121 ] 1.0 False {} 1\n",
      "10 [ 0.01113217  0.22242545 -0.02731172 -0.374389  ] 1.0 False {} 0\n",
      "11 [ 0.01558068  0.02770188 -0.0347995  -0.09044122] 1.0 False {} 0\n",
      "12 [ 0.01613471 -0.16690443 -0.03660833  0.19106254] 1.0 False {} 0\n",
      "13 [ 0.01279662 -0.36148408 -0.03278708  0.47197598] 1.0 False {} 0\n",
      "14 [ 0.00556694 -0.55612797 -0.02334756  0.75414723] 1.0 False {} 0\n",
      "15 [-0.00555562 -0.75092036 -0.00826461  1.0393927 ] 1.0 False {} 0\n",
      "16 [-0.02057402 -0.5556896   0.01252324  0.74412674] 1.0 False {} 1\n",
      "17 [-0.03168781 -0.3607427   0.02740578  0.45541105] 1.0 False {} 1\n",
      "18 [-0.03890267 -0.16601874  0.036514    0.17149109] 1.0 False {} 1\n",
      "19 [-0.04222304  0.02856208  0.03994382 -0.10945287] 1.0 False {} 1\n",
      "20 [-0.0416518  -0.16710882  0.03775476  0.19555973] 1.0 False {} 0\n",
      "21 [-0.04499398 -0.3627499   0.04166596  0.4999095 ] 1.0 False {} 0\n",
      "22 [-0.05224898 -0.5584337   0.05166414  0.8054267 ] 1.0 False {} 0\n",
      "23 [-0.06341765 -0.7542244   0.06777268  1.1139034 ] 1.0 False {} 0\n",
      "24 [-0.07850214 -0.5600546   0.09005075  0.84322727] 1.0 False {} 1\n",
      "25 [-0.08970323 -0.75628257  0.1069153   1.1628157 ] 1.0 False {} 0\n",
      "26 [-0.10482889 -0.5627028   0.13017161  0.9054773 ] 1.0 False {} 1\n",
      "27 [-0.11608294 -0.7593244   0.14828116  1.2360768 ] 1.0 False {} 0\n",
      "28 [-0.13126943 -0.56638587  0.17300269  0.99328   ] 1.0 False {} 1\n",
      "29 [-0.14259714 -0.37394735  0.19286829  0.7595407 ] 1.0 False {} 1\n",
      "30 [-0.15007609 -0.57112926  0.2080591   1.1061819 ] 1.0 False {} 0\n",
      "31 [-0.16149868 -0.37925828  0.23018274  0.8853113 ] 1.0 True {} 1\n",
      "max_t: 46\n"
     ]
    }
   ],
   "source": [
    "Random_games(10) # max_t = 45 = not great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>USE KERAS / NEURAL NET</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "# from keras.optimizers import Adam, RMSprop\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# # Neural Network model for Deep Q Learning\n",
    "# def OurModel(input_shape, action_space):\n",
    "\n",
    "#     X_input = Input(input_shape)\n",
    "\n",
    "#     # 'Dense' is the basic form of a neural network layer\n",
    "#     # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "#     X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "\n",
    "#     # Hidden layer with 256 nodes\n",
    "#     X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    \n",
    "#     # Hidden layer with 64 nodes\n",
    "#     X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "#     # Output Layer with # of actions: 2 nodes (left, right)\n",
    "#     X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "#     model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
    "    \n",
    "#     model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "#     model.summary()\n",
    "    \n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "def OurModel(input_shape, action_space):\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(256, activation='relu', input_shape=input_shape, kernel_initializer='he_uniform'),\n",
    "        layers.Dense(256, activation='relu', input_shape=input_shape, kernel_initializer='he_uniform'),\n",
    "        layers.Dense(256, activation='relu', kernel_initializer='he_uniform'),\n",
    "        layers.Dense(action_space, activation='linear', kernel_initializer='he_uniform'),\n",
    "    ],  )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = keras.optimizers.RMSprop\n",
    "        (\n",
    "            learning_rate=0.00016, \n",
    "            # decay = 0.001,\n",
    "            rho=0.95, \n",
    "            epsilon=0.01\n",
    "        ), \n",
    "        metrics=[\"accuracy\"],\n",
    "        loss='mse',\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a NN to understand and predict based on the environment data, we have initialized our model (will show it in original code) and feed it the information. Then the model will train on those data to approximate the output based on the input. Later in the complete code, you will see that the `fit()` method provides input and output pairs to the model.\n",
    "\n",
    "In the above model, I used three layers of Neural Network, `512, 256, and 64` neurons. Feel free to play with its structure and parameters.\n",
    "\n",
    "Later in the training process, you will see what makes the NN predict the reward value from a particular state. You will see that in code, I will use `model.fit(next_state, reward)`, same as in the standard Keras NN model.\n",
    "\n",
    "After training, the model we will be able to predict the output from unseen input. When we call `predict()` function on the model, the model will predict the reward of the current state based on the data we trained. Like so: `prediction = model.predict(next_state)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Implimenting Deep Q Network</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, in games, the reward directly relates to the score of the game. But, imagine a situation where the pole from the CartPole game is tilted to the left. The expected future reward of pushing the left button will then be higher than that of pushing the right button since it could yield a higher score of the game as the pole survives longer.\n",
    "\n",
    "To logically represent this intuition and train it, we need to express this as a formula to optimize. The loss is just a value that indicates how far our prediction is from the actual target. For example, the model's prediction could suggest more value in pushing the left button to gain more reward by pressing the right button. We want to decrease this gap between the prediction and the target (loss). So, we will define our loss function as follows:\n",
    "\n",
    "![](loss.png)\n",
    "\n",
    "\n",
    "We first act `a` and observe the reward `r` and resulting new state `s'`. Based on the result, we calculate the maximum target `Q(s', a')` and then discount it to make the future reward worth less than the immediate reward. Lastly, we add the current reward to the discounted future reward to get the target value. Subtracting our current prediction from the target gives the loss. Squaring this value allows us to punish the large loss value more and treat the negative values as positive ones.\n",
    "\n",
    "But it's not that difficult than you think it is; Keras takes care of most of the difficult tasks for us. \n",
    "\n",
    "We need to **define our target**. \n",
    "\n",
    "We can express the target in a magical one line of code in python: `target = reward + gamma * np.max(model.predict(next_state))`\n",
    "\n",
    "Keras does all the work of subtracting the target from the NN output and squaring it. It also applies the learning rate that we can define when creating the neural network model (otherwise, the model will determine it by itself); all this happens inside the `fit()` function. \n",
    "\n",
    "This function decreases the gap between our prediction to target by the learning rate. The approximation of the Q-value converges to the true Q-value as we tend to repeat the change method. The loss decreases, and therefore the score grows higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>MEMORY AND REPLAY</ins>\n",
    "\n",
    "The most notable features of the DQN algorithm are \"remembered\" and \"replay\" methods. Both are simple concepts. The original DQN design contains a lot of tweaks for a better learning process. However, we tend to stick to a less complicated version for better understanding.\n",
    "\n",
    "<ins>REMEMBER</ins>\n",
    "\n",
    "One of the specific things for DQN is that the Neural Network used in the algorithm tends to forget the previous experiences as it overwrites them with new experiences. \n",
    "\n",
    "So, we need a memory (list) of previous experiences and observations to re-train the model with the earlier experiences. \n",
    "\n",
    "Experience replay could be named a biologically inspired method that uniformly (scales back the correlation between sequence actions) samples experiences from the memory and updates its Q values for every entry. We will call this array of experiences memory and use a `remember()` function to `append state, action, reward, and next state `to the memory.\n",
    "\n",
    "In our example, the memory list will have a form of:\n",
    "\n",
    "`memory = [(state, action, reward, next_state, done)...]`\n",
    "\n",
    "And remember function will store states, actions, and resulting rewards to the memory like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR USE IN AGENT CLASS\n",
    "def remember(self, state, action, reward, next_state, done):\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "    if len(self.memory) > self.train_start:\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the agent perform well in the long term, we need to consider the immediate rewards and the future rewards we will get. To do this, we will have a discount rate or gamma and ultimately add it to the current state reward. This way, the agent will learn to maximize the discounted future reward based on the given state. \n",
    "\n",
    "In other words, we are updating our `Q value` with the *cumulative discounted future rewards*.\n",
    "\n",
    "`done` is just a Boolean that indicates if the state is the final state (cartpole failed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>REPLAY</ins>\n",
    "\n",
    "A method that trains NN with experiences in the memory we will call replay() function. First, we will sample some experiences from the memory and call them minibatch.\n",
    "\n",
    "` minibatch = random.sample(memory, min(len(memory), batch_size))`\n",
    "\n",
    "The above code will make a minibatch, just randomly sampled elements from full memories of size batch_size. I will set the batch size as 64 for this example. If the memory size is less than 64, we will take everything is in our memory.\n",
    "\n",
    "For those of you who wonder how such function can converge, as it looks like it is trying to predict its output (in some sense it is), don't worry — it's possible, and in our simple case, it does. However, convergence is not always that 'easy', and in more complex problems, there comes a need for more advanced techniques than CartPole stabilize training. For example, these techniques are Double DQN's or Dueling DQN's, but that's a topic for another article (stay tuned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR USE IN AGENT CLASS\n",
    "\n",
    "def replay(self):\n",
    "    if len(self.memory) < self.train_start:\n",
    "        return\n",
    "    # Randomly sample minibatch from the memory\n",
    "    minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "    state = np.zeros((self.batch_size, self.state_size))\n",
    "    next_state = np.zeros((self.batch_size, self.state_size))\n",
    "    action, reward, done = [], [], []\n",
    "\n",
    "    # do this before prediction\n",
    "    # for speedup, this could be done on the tensor level\n",
    "    # but easier to understand using a loop\n",
    "    for i in range(self.batch_size):\n",
    "        state[i] = minibatch[i][0]\n",
    "        action.append(minibatch[i][1])\n",
    "        reward.append(minibatch[i][2])\n",
    "        next_state[i] = minibatch[i][3]\n",
    "        done.append(minibatch[i][4])\n",
    "\n",
    "    # do batch prediction to save speed\n",
    "    target = self.model.predict(state)\n",
    "    target_next = self.model.predict(next_state)\n",
    "\n",
    "    for i in range(self.batch_size):\n",
    "        # correction on the Q value for the action used\n",
    "        if done[i]:\n",
    "            target[i][action[i]] = reward[i]\n",
    "        else:\n",
    "            # Standard - DQN\n",
    "            # DQN chooses the max Q value among next actions\n",
    "            # selection and evaluation of action is on the target Q Network\n",
    "            # Q_max = max_a' Q_target(s', a')\n",
    "            target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "    # Train the Neural Network with batches\n",
    "    self.model.fit(state, target, batch_size=self.batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>SETTING HYPER PARAMETERS</ins>\n",
    "\n",
    "There are some parameters that have to be passed to a reinforcement learning agent. You will see similar parameters in all DQN models:\n",
    "\n",
    "`EPISODES` — number of games we want the agent to play;\n",
    "\n",
    "`Gamma` — decay or discount rate, to calculate the future discounted reward;\n",
    "\n",
    "`epsilon` — exploration rate is the rate in which an agent randomly decides its action rather than a prediction;\n",
    "\n",
    "`epsilon_decay` — we want to decrease the number of explorations as it gets good at playing games;\n",
    "\n",
    "`epsilon_min` — we want the agent to explore at least this amount;\n",
    "\n",
    "`learning_rate` — Determines how much neural net learns in each iteration (if used);\n",
    "\n",
    "`batch_size` — Determines how much memory DQN will use to train;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[ 0.04206469  0.0359985  -0.01194496  0.02282057]\n",
      "[[ 0.04206469  0.0359985  -0.01194496  0.02282057]]\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n)\n",
    "state = env.reset()\n",
    "state_size = env.observation_space.shape[0] # 4\n",
    "print(state)\n",
    "\n",
    "state = np.reshape(state, [1, state_size])\n",
    "\n",
    "print(state)\n",
    "print(env._max_episode_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad hyper-parameters\n",
    "hparams = {\n",
    "    'learning_rate': 0.000171782176772688,\n",
    "    'discount_factor': 0.95,\n",
    "    'batch_size': 32,\n",
    "    'memory_size': 10000,\n",
    "    'freq_steps_train': 8,\n",
    "    'freq_steps_update_target': 10,\n",
    "    'n_steps_warm_up_memory': 1000,\n",
    "    'n_gradient_steps': 4,\n",
    "    'nn_hidden_layers': [256, 256],\n",
    "    'max_grad_norm': 10,\n",
    "    'normalize_state': False,\n",
    "    'epsilon_start': 0.9,\n",
    "    'epsilon_end': 0.19818684841911674,\n",
    "    'steps_epsilon_decay': 1000,\n",
    "}\n",
    "\n",
    "# Good hyper-parameters\n",
    "# make you feel great!\n",
    "hparams = {\n",
    "    'learning_rate': 0.00016151809562265122,\n",
    "    'discount_factor': 0.99,\n",
    "    'batch_size': 32,\n",
    "    'memory_size': 10000,\n",
    "    'freq_steps_train': 8,\n",
    "    'freq_steps_update_target': 10,\n",
    "    'n_steps_warm_up_memory': 1000,\n",
    "    'n_gradient_steps': 16,\n",
    "    'nn_hidden_layers': [256, 256],\n",
    "    'max_grad_norm': 10,\n",
    "    'normalize_state': False,\n",
    "    'epsilon_start': 0.9,\n",
    "    'epsilon_end': 0.14856584122699473,\n",
    "    'steps_epsilon_decay': 10000,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHOLE SHEBANG\n",
    "\n",
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import random \n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        # by default, CartPole-v1 has max episode steps = 500\n",
    "        self.state_size = self.env.observation_space.shape[0] # 4\n",
    "        self.action_size = self.env.action_space.n # 2\n",
    "        self.EPISODES = 1000\n",
    "        self.memory = deque(maxlen=2000) # double ended quee quick popping and appending\n",
    "        \n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0 # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # create main model\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        stores an epidose in memory\n",
    "        if memory has more than 1000 episodes in it AND epsilon is above min, decays the epsilon\n",
    "        '''\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.train_start:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def act(self, state):\n",
    "        '''\n",
    "        deifnes explore/don't explore. \n",
    "        if random num between 0 and 1 is less than epsilon will exlore \n",
    "        if it's not (increasinly likely as time goes on) \n",
    "        act will select the action which the NN says is the best value given the state\n",
    "        '''\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self):\n",
    "        '''\n",
    "        if memory is less than 1000 observations does nothing\n",
    "        otherwise\n",
    "        if memory is more than 1000 observartions\n",
    "        takes a random saple from memory of length whichever is smaller - length of the memory, or the batch size. So if not enough in memory, takes whole thing, otherwise takes bacth size\n",
    "\n",
    "        then......\n",
    "        creates a minibatch of 64, which is then splits into diffrent matrixes for state, action, reward, next_state and done\n",
    "        it then uses the model to generate Qvalues for all of the 64 states\n",
    "        it then updates these target values using the formula Qval(current_s, a_taken) = r + gamma * maxQval(next_state, maximising_a)\n",
    "        where the maxQval(next_state, maximising_a) is found from the model doing a prediction on all the next_states and taking the max_val for each entry\n",
    "        then updates the NN with the updated targets and states\n",
    "        '''\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "\n",
    "        # Randomly sample minibatch from the memory takes a random saple from memory of length whichever is smaller - length of the memory, or the batch size. So if not enough in memory, takes whole thing, otherwise takes batch size\n",
    "        # batch size is 64\n",
    "        # minibatch is therefor a 64 entry long list with each entry 5 values long like:\n",
    "        # [\n",
    "        # [s, a, r, s', d], \n",
    "        # [s, a, r, s', d], \n",
    "        # [s, a, r, s', d], \n",
    "        # [s, a, r, s', d], \n",
    "        # [s, a, r, s', d], \n",
    "        # ... \n",
    "        # ]\n",
    "        # where s and s' are 4 number lists [x, v, t, o]\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "\n",
    "        # create state matrix (64 x 4) all zeros\n",
    "        # [\n",
    "            # [0, 0, 0, 0],\n",
    "            # [0, 0, 0, 0],\n",
    "            # [0, 0, 0, 0],\n",
    "            # [0, 0, 0, 0],\n",
    "            # ...\n",
    "        # ]\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        # create next_state matrix (64 x 4)\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        # create empty arrays for action, reward done\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for i in range(self.batch_size):\n",
    "            # for i 0 to 64\n",
    "\n",
    "            # set state[0] to be minibatch[0][0]\n",
    "            # which means set state[i] to be minibatch[i][s]\n",
    "            state[i] = minibatch[i][0]\n",
    "\n",
    "            # add minibatch[i][a] to action\n",
    "            action.append(minibatch[i][1])\n",
    "            # add minibatch[i][r] to reward\n",
    "            reward.append(minibatch[i][2])\n",
    "\n",
    "            # set next_state[0] to minibatch[0][s']\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            # append minibatch[i][d] to done\n",
    "            done.append(minibatch[i][4])\n",
    "        # loop gives us state of length 64: \n",
    "        # [\n",
    "        # [state from minibatch[0]], \n",
    "        # [state from minibatch[1]], \n",
    "        # [state from minibatch[2]],\n",
    "        # ...[state from minibatch[63]]\n",
    "        # ]  \n",
    "        # loop gives us action length 64: \n",
    "        # [0, 1, 1, 0, 1, 0, 0,...1]\n",
    "        # loop gives us reward length 64: \n",
    "        # [1, 1, 1, 1, -100, 1, 1,...1]\n",
    "        # loop gives us done length 64: \n",
    "        # [False, False, False, False, True, False, False,...False]  \n",
    "        # # loop gives us next_state of length 64: \n",
    "        # [\n",
    "        # [next_state from minibatch[0]], \n",
    "        # [next_state from minibatch[1]], \n",
    "        # [next_state from minibatch[2]],\n",
    "        # ...[next_state from minibatch[63]]\n",
    "        # ]    \n",
    "\n",
    "\n",
    "        # get the models Q values for actions in our current state returns I guess a 64 x 2 array:\n",
    "        #  [\n",
    "        # [QValA1, QValA2], <- for episode 0 state\n",
    "        # [QValA1, QValA2], <- for episode 1 state\n",
    "        # [QValA1, QValA2], <- for episode 2 state\n",
    "        # [QValA1, QValA2], <- for episode 3 state\n",
    "        # ]\n",
    "        target = self.model.predict(state)\n",
    "\n",
    "        # get the models Q values for actions for our next_state\n",
    "        target_next = self.model.predict(next_state)\n",
    "        #  [\n",
    "        # [QValA1, QValA2], <- for next_state from episode 0 state\n",
    "        # [QValA1, QValA2], <- for next_state from episode 1 state\n",
    "        # [QValA1, QValA2], <- for next_state from episode 2 state\n",
    "        # [QValA1, QValA2], <- for next_state from episode 3 state\n",
    "        # ]\n",
    "\n",
    "        # for each episode in the batch\n",
    "        # if the episode was one in which the episode was completed, correct the target for that state to the reward in the original rules\n",
    "        # otherwise\n",
    "        # we set the target to the reward + gamma x the models max value prediction for the next state\n",
    "        for i in range(self.batch_size):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # Standard - DQN\n",
    "                # DQN chooses the max Q value among next actions\n",
    "                # selection and evaluation of action is on the target Q Network\n",
    "                # Q_max = max_a' Q_target(s', a')\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches using the state matrix and the target matrix\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "        # print(\"FITTING NN\")\n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        '''\n",
    "        load a model by name\n",
    "        '''\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        '''\n",
    "        save a model by name\n",
    "        '''\n",
    "        self.model.save(name)\n",
    "            \n",
    "    def run(self):\n",
    "    # for each of 1000 episodes\n",
    "      results = []\n",
    "      five_hundreds = 0\n",
    "      for e in range(self.EPISODES):\n",
    "\n",
    "          # initialise random state\n",
    "          state = self.env.reset()\n",
    "          # resizes random state from [x, v, t, o] to [[x, v, t, o]]\n",
    "          state = np.reshape(state, [1, self.state_size])\n",
    "          done = False\n",
    "\n",
    "          # keep track of iteration\n",
    "          i = 0\n",
    "          # as long as the cart has not falled or been completed (i.e. as long as the ste doesnt return done = True), continue to loop within the episode\n",
    "          # once done is true either start new episode or if i = 500, save the model\n",
    "          # unless i = 500 for every action run replay() ()see below\n",
    "          while not done:\n",
    "              # render env\n",
    "              # self.env.render()\n",
    "\n",
    "              # pass state to act() function. It Returns action to take depending on epsilon - either random, or best option the NN says\n",
    "              action = self.act(state)\n",
    "\n",
    "              # take the action specified and collect the (next_state, reward, done) to be stored with the current state so we get (state, action, reward, next_state)\n",
    "              next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "              # reshape next_state from [x, v, t, o] (4) to [[x, v, t, o]] (1, 4)\n",
    "              next_state = np.reshape(next_state, [1, self.state_size])\n",
    "\n",
    "              # if done False or i == 499 reward = given reward \n",
    "              if not done or i == self.env._max_episode_steps-1:\n",
    "                  reward = reward\n",
    "              # if done is true reward = -100    \n",
    "              else:\n",
    "                  reward = -100\n",
    "\n",
    "              # pass [original_state, action, reward, next_state, done_status] to the remember() function\n",
    "              # it appends tuple: (state, action, reward, next_state, done) to memory and, if memory full enough decays the epsilon\n",
    "              self.remember(state, action, reward, next_state, done)\n",
    "\n",
    "              # update state variable to be the next state\n",
    "              state = next_state\n",
    "\n",
    "              # 3 incriment the iteration var i\n",
    "              i += 1\n",
    "\n",
    "              if done:\n",
    "                  # if done then display episode/no of episodes, score(i) as in how many actions before done, then the final epsilon value                    \n",
    "                  print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.EPISODES, i, self.epsilon))\n",
    "                  results.append([e, i])\n",
    "                  if i == 500:\n",
    "                      five_hundreds += 1\n",
    "                      # if 500 reached (i.e. game completed) save the model\n",
    "                  if five_hundreds == 12:\n",
    "                      print(\"Saving trained model as cartpole-dqn.h5\")\n",
    "                      self.save(\"cartpole-dqn.h5\")\n",
    "                      # exit out of loop\n",
    "                      return results\n",
    "              # calls replay function after every observation, once there are 1000 observations in memory it does a mini batch of 64 to update the model\n",
    "              self.replay()\n",
    "\n",
    "    def test(self):\n",
    "        self.load(\"cartpole-dqn.h5\")\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                # self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_39 (Dense)            (None, 256)               1280      \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 133,378\n",
      "Trainable params: 133,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0/1000, score: 15, e: 1.0\n",
      "episode: 1/1000, score: 33, e: 1.0\n",
      "episode: 2/1000, score: 23, e: 1.0\n",
      "episode: 3/1000, score: 27, e: 1.0\n",
      "episode: 4/1000, score: 25, e: 1.0\n",
      "episode: 5/1000, score: 24, e: 1.0\n",
      "episode: 6/1000, score: 34, e: 1.0\n",
      "episode: 7/1000, score: 25, e: 1.0\n",
      "episode: 8/1000, score: 25, e: 1.0\n",
      "episode: 9/1000, score: 21, e: 1.0\n",
      "episode: 10/1000, score: 12, e: 1.0\n",
      "episode: 11/1000, score: 31, e: 1.0\n",
      "episode: 12/1000, score: 20, e: 1.0\n",
      "episode: 13/1000, score: 21, e: 1.0\n",
      "episode: 14/1000, score: 31, e: 1.0\n",
      "episode: 15/1000, score: 16, e: 1.0\n",
      "episode: 16/1000, score: 23, e: 1.0\n",
      "episode: 17/1000, score: 13, e: 1.0\n",
      "episode: 18/1000, score: 15, e: 1.0\n",
      "episode: 19/1000, score: 25, e: 1.0\n",
      "episode: 20/1000, score: 15, e: 1.0\n",
      "episode: 21/1000, score: 21, e: 1.0\n",
      "episode: 22/1000, score: 12, e: 1.0\n",
      "episode: 23/1000, score: 55, e: 1.0\n",
      "episode: 24/1000, score: 41, e: 1.0\n",
      "episode: 25/1000, score: 38, e: 1.0\n",
      "episode: 26/1000, score: 13, e: 1.0\n",
      "episode: 27/1000, score: 9, e: 1.0\n",
      "episode: 28/1000, score: 11, e: 1.0\n",
      "episode: 29/1000, score: 18, e: 1.0\n",
      "episode: 30/1000, score: 12, e: 1.0\n",
      "episode: 31/1000, score: 21, e: 1.0\n",
      "episode: 32/1000, score: 17, e: 1.0\n",
      "episode: 33/1000, score: 51, e: 1.0\n",
      "episode: 34/1000, score: 14, e: 1.0\n",
      "episode: 35/1000, score: 51, e: 1.0\n",
      "episode: 36/1000, score: 26, e: 1.0\n",
      "episode: 37/1000, score: 11, e: 1.0\n",
      "episode: 38/1000, score: 12, e: 1.0\n",
      "episode: 39/1000, score: 18, e: 1.0\n",
      "episode: 40/1000, score: 12, e: 1.0\n",
      "episode: 41/1000, score: 19, e: 1.0\n",
      "episode: 42/1000, score: 17, e: 1.0\n",
      "episode: 43/1000, score: 12, e: 1.0\n",
      "episode: 44/1000, score: 11, e: 1.0\n",
      "episode: 45/1000, score: 13, e: 0.99\n",
      "episode: 46/1000, score: 16, e: 0.98\n",
      "episode: 47/1000, score: 10, e: 0.97\n",
      "episode: 48/1000, score: 21, e: 0.95\n",
      "episode: 49/1000, score: 12, e: 0.93\n",
      "episode: 50/1000, score: 52, e: 0.89\n",
      "episode: 51/1000, score: 21, e: 0.87\n",
      "episode: 52/1000, score: 24, e: 0.85\n",
      "episode: 53/1000, score: 32, e: 0.82\n",
      "episode: 54/1000, score: 10, e: 0.81\n",
      "episode: 55/1000, score: 27, e: 0.79\n",
      "episode: 56/1000, score: 25, e: 0.77\n",
      "episode: 57/1000, score: 46, e: 0.74\n",
      "episode: 58/1000, score: 80, e: 0.68\n",
      "episode: 59/1000, score: 21, e: 0.67\n",
      "episode: 60/1000, score: 59, e: 0.63\n",
      "episode: 61/1000, score: 64, e: 0.59\n",
      "episode: 62/1000, score: 52, e: 0.56\n",
      "episode: 63/1000, score: 83, e: 0.51\n",
      "episode: 64/1000, score: 131, e: 0.45\n",
      "episode: 65/1000, score: 161, e: 0.38\n",
      "episode: 66/1000, score: 251, e: 0.3\n",
      "episode: 67/1000, score: 235, e: 0.24\n",
      "episode: 68/1000, score: 361, e: 0.16\n",
      "episode: 69/1000, score: 318, e: 0.12\n",
      "episode: 70/1000, score: 278, e: 0.091\n",
      "episode: 71/1000, score: 288, e: 0.068\n",
      "episode: 72/1000, score: 225, e: 0.054\n",
      "episode: 73/1000, score: 280, e: 0.041\n",
      "episode: 74/1000, score: 257, e: 0.032\n",
      "episode: 75/1000, score: 234, e: 0.025\n",
      "episode: 76/1000, score: 265, e: 0.019\n",
      "episode: 77/1000, score: 227, e: 0.015\n",
      "episode: 78/1000, score: 276, e: 0.012\n",
      "episode: 79/1000, score: 308, e: 0.0086\n",
      "episode: 80/1000, score: 248, e: 0.0067\n",
      "episode: 81/1000, score: 229, e: 0.0053\n",
      "episode: 82/1000, score: 279, e: 0.004\n",
      "episode: 83/1000, score: 255, e: 0.0031\n",
      "episode: 84/1000, score: 318, e: 0.0023\n",
      "episode: 85/1000, score: 271, e: 0.0017\n",
      "episode: 86/1000, score: 264, e: 0.0013\n",
      "episode: 87/1000, score: 303, e: 0.001\n",
      "episode: 88/1000, score: 473, e: 0.001\n",
      "episode: 89/1000, score: 411, e: 0.001\n",
      "episode: 90/1000, score: 213, e: 0.001\n",
      "episode: 91/1000, score: 302, e: 0.001\n",
      "episode: 92/1000, score: 290, e: 0.001\n",
      "episode: 93/1000, score: 198, e: 0.001\n",
      "episode: 94/1000, score: 244, e: 0.001\n",
      "episode: 95/1000, score: 268, e: 0.001\n",
      "episode: 96/1000, score: 360, e: 0.001\n",
      "episode: 97/1000, score: 225, e: 0.001\n",
      "episode: 98/1000, score: 351, e: 0.001\n",
      "episode: 99/1000, score: 204, e: 0.001\n",
      "episode: 100/1000, score: 400, e: 0.001\n",
      "episode: 101/1000, score: 240, e: 0.001\n",
      "episode: 102/1000, score: 268, e: 0.001\n",
      "episode: 103/1000, score: 255, e: 0.001\n",
      "episode: 104/1000, score: 195, e: 0.001\n",
      "episode: 105/1000, score: 229, e: 0.001\n",
      "episode: 106/1000, score: 272, e: 0.001\n",
      "episode: 107/1000, score: 248, e: 0.001\n",
      "episode: 108/1000, score: 260, e: 0.001\n",
      "episode: 109/1000, score: 250, e: 0.001\n",
      "episode: 110/1000, score: 261, e: 0.001\n",
      "episode: 111/1000, score: 258, e: 0.001\n",
      "episode: 112/1000, score: 285, e: 0.001\n",
      "episode: 113/1000, score: 287, e: 0.001\n",
      "episode: 114/1000, score: 206, e: 0.001\n",
      "episode: 115/1000, score: 235, e: 0.001\n",
      "episode: 116/1000, score: 170, e: 0.001\n",
      "episode: 117/1000, score: 188, e: 0.001\n",
      "episode: 118/1000, score: 9, e: 0.001\n",
      "episode: 119/1000, score: 11, e: 0.001\n",
      "episode: 120/1000, score: 10, e: 0.001\n",
      "episode: 121/1000, score: 10, e: 0.001\n",
      "episode: 122/1000, score: 10, e: 0.001\n",
      "episode: 123/1000, score: 225, e: 0.001\n",
      "episode: 124/1000, score: 221, e: 0.001\n",
      "episode: 125/1000, score: 188, e: 0.001\n",
      "episode: 126/1000, score: 223, e: 0.001\n",
      "episode: 127/1000, score: 195, e: 0.001\n",
      "episode: 128/1000, score: 188, e: 0.001\n",
      "episode: 129/1000, score: 174, e: 0.001\n",
      "episode: 130/1000, score: 194, e: 0.001\n",
      "episode: 131/1000, score: 209, e: 0.001\n",
      "episode: 132/1000, score: 260, e: 0.001\n",
      "episode: 133/1000, score: 300, e: 0.001\n",
      "episode: 134/1000, score: 286, e: 0.001\n",
      "episode: 135/1000, score: 248, e: 0.001\n",
      "episode: 136/1000, score: 275, e: 0.001\n",
      "episode: 137/1000, score: 266, e: 0.001\n",
      "episode: 138/1000, score: 257, e: 0.001\n",
      "episode: 139/1000, score: 317, e: 0.001\n",
      "episode: 140/1000, score: 344, e: 0.001\n",
      "episode: 141/1000, score: 499, e: 0.001\n",
      "episode: 142/1000, score: 416, e: 0.001\n",
      "episode: 143/1000, score: 500, e: 0.001\n",
      "episode: 144/1000, score: 500, e: 0.001\n",
      "episode: 145/1000, score: 500, e: 0.001\n",
      "episode: 146/1000, score: 500, e: 0.001\n",
      "episode: 147/1000, score: 500, e: 0.001\n",
      "episode: 148/1000, score: 500, e: 0.001\n",
      "episode: 149/1000, score: 500, e: 0.001\n",
      "episode: 150/1000, score: 500, e: 0.001\n",
      "episode: 151/1000, score: 500, e: 0.001\n",
      "episode: 152/1000, score: 490, e: 0.001\n",
      "episode: 153/1000, score: 413, e: 0.001\n",
      "episode: 154/1000, score: 500, e: 0.001\n",
      "episode: 155/1000, score: 500, e: 0.001\n",
      "episode: 156/1000, score: 500, e: 0.001\n",
      "Saving trained model as cartpole-dqn.h5\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "results = agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 15], [1, 33], [2, 23], [3, 27], [4, 25], [5, 24], [6, 34], [7, 25], [8, 25], [9, 21], [10, 12], [11, 31], [12, 20], [13, 21], [14, 31], [15, 16], [16, 23], [17, 13], [18, 15], [19, 25], [20, 15], [21, 21], [22, 12], [23, 55], [24, 41], [25, 38], [26, 13], [27, 9], [28, 11], [29, 18], [30, 12], [31, 21], [32, 17], [33, 51], [34, 14], [35, 51], [36, 26], [37, 11], [38, 12], [39, 18], [40, 12], [41, 19], [42, 17], [43, 12], [44, 11], [45, 13], [46, 16], [47, 10], [48, 21], [49, 12], [50, 52], [51, 21], [52, 24], [53, 32], [54, 10], [55, 27], [56, 25], [57, 46], [58, 80], [59, 21], [60, 59], [61, 64], [62, 52], [63, 83], [64, 131], [65, 161], [66, 251], [67, 235], [68, 361], [69, 318], [70, 278], [71, 288], [72, 225], [73, 280], [74, 257], [75, 234], [76, 265], [77, 227], [78, 276], [79, 308], [80, 248], [81, 229], [82, 279], [83, 255], [84, 318], [85, 271], [86, 264], [87, 303], [88, 473], [89, 411], [90, 213], [91, 302], [92, 290], [93, 198], [94, 244], [95, 268], [96, 360], [97, 225], [98, 351], [99, 204], [100, 400], [101, 240], [102, 268], [103, 255], [104, 195], [105, 229], [106, 272], [107, 248], [108, 260], [109, 250], [110, 261], [111, 258], [112, 285], [113, 287], [114, 206], [115, 235], [116, 170], [117, 188], [118, 9], [119, 11], [120, 10], [121, 10], [122, 10], [123, 225], [124, 221], [125, 188], [126, 223], [127, 195], [128, 188], [129, 174], [130, 194], [131, 209], [132, 260], [133, 300], [134, 286], [135, 248], [136, 275], [137, 266], [138, 257], [139, 317], [140, 344], [141, 499], [142, 416], [143, 500], [144, 500], [145, 500], [146, 500], [147, 500], [148, 500], [149, 500], [150, 500], [151, 500], [152, 490], [153, 413], [154, 500], [155, 500], [156, 500]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(results)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"performance over time\" )    \n",
    "pd.Series([result[1] for result in results]).plot(kind='line')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: 500\n",
      "episode: 1/1000, score: 500\n",
      "episode: 2/1000, score: 500\n",
      "episode: 3/1000, score: 500\n",
      "episode: 4/1000, score: 500\n",
      "episode: 5/1000, score: 500\n",
      "episode: 6/1000, score: 500\n",
      "episode: 7/1000, score: 500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bl/31xztvt94dv9bx23976lfvxh0000gn/T/ipykernel_14119/4010618939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/bl/31xztvt94dv9bx23976lfvxh0000gn/T/ipykernel_14119/3252170022.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;31m# self.env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1980\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1982\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1983\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "\n",
    "if not done:\n",
    "    print(\"1\")\n",
    "else:\n",
    "    print(\"2\")\n",
    "\n",
    "\n",
    "example = [[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4] ]\n",
    "minibatch = random.sample(example, 3)\n",
    "print(len(minibatch))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
