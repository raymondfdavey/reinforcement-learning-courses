{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym # contains out Taxi v3 environment\n",
    "import random\n",
    "\n",
    "# good walkthrough: https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "# original google colab: https://colab.research.google.com/gist/simoninithomas/466c81aa1c2a07dd14793240c6d033c5/q-learning-with-taxi-v3.ipynb#scrollTo=20tSdDbxxK_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OpenAI gym\n",
    "\n",
    "It has this exact environment already built for us.\n",
    "\n",
    "Gym provides different game environments which we can plug into our code and test an agent. The library takes care of API for providing all the information that our agent would require, like possible actions, score, and current state. We just need to focus just on the algorithm part for our agent.\n",
    "\n",
    "We'll be using the Gym environment called Taxi-V3, which all of the details explained above were pulled from. The objectives, rewards, and actions are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create environment and render it so we can see it\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our environment looks like this:\n",
    "\n",
    "It's a 5x5 grid world\n",
    "Our taxi is spawned randomly in a square.\n",
    "The passenger is spawned randomly in one of the 4 possible locations (R, B, G, Y) and wishes to go in one of the 4 possibles locations too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  500  possible states\n",
      "There are  6  possible actions\n"
     ]
    }
   ],
   "source": [
    "# investigate observation space and actions space\n",
    "\n",
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE SPACE\n",
    "\n",
    "# The State Space is the set of all possible situations our taxi could inhabit. The state should contain useful information the agent needs to make the right action.\n",
    "# 500 possible states that the board can be in as this includes all combinations of passenger locations, destination locations and 5 x 5 grid that the car could be in = 5 * 5 * 5 * 4 = 500\n",
    "\n",
    "# ACTION SPACE\n",
    "\n",
    "# The agent encounters one of the 500 states and it takes an action. \n",
    "# The action in our case can be to move in a direction or decide to pickup/dropoff a passenger.\n",
    "# 6 poss actions: SOUTH NORTH EAST WEST PICKUP DROP-OFF\n",
    "\n",
    "# the taxi cannot perform certain actions in certain states due to walls. In the environment's code, we will simply provide a -1 penalty for every wall hit and the taxi won't move anywhere. This will just rack up penalties causing the taxi to consider going around the wall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AS RL works by rewards lets have a look at them:\n",
    "### Rewards\n",
    "Since the agent (the imaginary driver) is reward-motivated and is going to learn how to control the cab by trial experiences in the environment, we need to decide the rewards and/or penalties and their magnitude accordingly. Here a few points to consider:\n",
    "\n",
    " - The agent should receive a high positive reward for a successful dropoff because this behavior is highly desired +20\n",
    " - The agent should be penalized if it tries to drop off a passenger in wrong locations -10\n",
    " - The agent should get a slight negative reward for not making it to the destination after every time-step. \"Slight\" negative because we would prefer our agent to reach late instead of making wrong moves trying to reach to the destination as fast as possible -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core gym interface is `env`, which is the unified environment interface. The following are the env methods that would be quite helpful to us:\n",
    "\n",
    " - `env.reset`: Resets the environment and returns a random initial state.\n",
    " - `env.step(action)`: Step the environment by one timestep. Returns\n",
    "    - **observation**: Observations of the environment\n",
    "    - **reward**: If your action was beneficial or not\n",
    "    - **done**: Indicates if we have successfully picked up and dropped off a passenger, also called one episode\n",
    "    - **info**: Additional info such as performance and latency for debugging purposes\n",
    " - `env.render`: Renders one frame of the environment (helpful in visualizing the environment)\n",
    "\n",
    "Note: We are using the `.env on` the end of `make` to avoid training stopping at 200 iterations, which is the default for the new version of Gym (reference).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder of our problem**\n",
    "\n",
    " - Here's our restructured problem statement (from Gym docs):\n",
    "\n",
    "*\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\"*\n",
    "\n",
    "Let's dive more into the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    " - The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    " - R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination.\n",
    " \n",
    " As verified by the prints, we have an Action Space of size 6 and a State Space of size 500. As you'll see, our RL algorithm won't need any more information than these two things. All we need is a way to identify a state uniquely by assigning a unique number to every possible state, and RL learns to choose an action number from 0-5 where:\n",
    "\n",
    " - 0 = south\n",
    " - 1 = north\n",
    " - 2 = east\n",
    " - 3 = west\n",
    " - 4 = pickup\n",
    " - 5 = dropoff\n",
    "\n",
    "Recall that the 500 states correspond to a encoding of the taxi's location, the passenger's location, and the destination location.\n",
    "\n",
    "Reinforcement Learning will learn a mapping of states to the optimal action to perform in that state by exploration, i.e. the agent explores the environment and takes actions based off rewards defined in the environment.\n",
    "\n",
    "The optimal action for each state is the action that has the highest cumulative long-term reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Reinforcement_Learning_Taxi_Env.width-1200.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually pass in the details shown in the pic above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.reset()\n",
    "\n",
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using our illustration's coordinates to generate a number corresponding to a state between 0 and 499, which turns out to be 328 for our illustration's state.\n",
    "\n",
    "Then we can set the environment's state manually with env.env.s using that encoded number. You can play around with the numbers and you'll see the taxi, passenger, and destination move around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.env.s = 123\n",
    "# env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reward Table (Q Table)\n",
    "When the Taxi environment is created, there is an initial Reward table that's also created, called `P`. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. a  matrix.\n",
    "\n",
    "Since every state is in this matrix, we can see the default reward values assigned to our illustration's state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]\n",
    "\n",
    "# {0: [(1.0, 428, -1, False)],\n",
    "#  1: [(1.0, 228, -1, False)],\n",
    "#  2: [(1.0, 348, -1, False)],\n",
    "#  3: [(1.0, 328, -1, False)],\n",
    "#  4: [(1.0, 328, -10, False)],\n",
    "#  5: [(1.0, 328, -10, False)]}\n",
    "\n",
    "# This dictionary has the structure: {action: [(probability, nextstate, reward, done)]}.\n",
    "# 0, 1, 2, 3, 4, 5 = DOWN, UP, RIGHT, LEFT, PICKUP, DROP-OFF \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary has the structure: `{action: [(probability, nextstate, reward, done)]}`.\n",
    "\n",
    "A few things to note:\n",
    "\n",
    " - The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.\n",
    " - In this `env`, probability is always 1.0.\n",
    " - The nextstate is the state we would be in if we take the action at this index of the dict\n",
    " - All the movement actions have a -1 reward and the pickup/dropoff actions have -10 reward in this particular state. If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5)\n",
    " - **done** is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode\n",
    "\n",
    "Note: if our agent chose to explore action two (2) in this state it would be going East into a wall. The source code has made it impossible to actually move the taxi across a wall, so if the taxi chooses that action, it will just keep accruing -1 penalties, which affects the **long-term reward**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the environment without Reinforcement Learning\n",
    "\n",
    "Let's see what would happen if we try to brute-force our way to solving the problem without RL.\n",
    "\n",
    "Since we have our P table for default rewards in each state, we can try to have our taxi navigate just using that.\n",
    "\n",
    "We'll create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 20. \n",
    "\n",
    "The `env.action_space.sample()` method automatically selects one random action from set of all possible actions.\n",
    "\n",
    "Let's see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 1\n",
      "Penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode=\"ansi\"),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs)) # 200\n",
    "print(\"Penalties incurred: {}\".format(penalties)) # 76\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  500  possible states\n",
      "There are  6  possible actions\n",
      "(500, 6)\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Implementing Q-learning in python\n",
    "# initialise Q-table with 500 x 6 matrix of zeroes\n",
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")\n",
    "\n",
    "\n",
    "q_table = np.zeros((state_space, action_space)) \n",
    "print(q_table.shape) # (500, 6)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE HYPERPARAMTERS\n",
    "\n",
    "total_episodes = 50000        # Total episodes\n",
    "total_test_episodes = 100     # Total test episodes\n",
    "max_steps = 99                # Max steps per episode\n",
    "\n",
    "learning_rate = 0.7           # Learning rate\n",
    "gamma = 0.618                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the epsilon-greedy policy\n",
    "\n",
    "Epsilon-Greedy is a policy that handles the exploration/[exploitation trade-off](https://medium.com/@thomassimonini/an-introduction-to-deep-reinforcement-learning-17a565999c0c?source=friends_link&sk=1b1121ae5d9814a09ca38b47abc7dc61).\n",
    "\n",
    "### The idea\n",
    "\n",
    "Epsilon Greedy:\n",
    "\n",
    "- We do **exploitation** (aka our agent selects the action with the highest state-action pair value) with a prob of *1 - ɛ* : \n",
    "- We do **exploration** (trying random action) *with probability ɛ*:\n",
    "\n",
    "As the training goes on, **we progressively reduce the epsilon value** since we will **need less and less exploration and more exploitation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the epsilon-greedy policy\n",
    "\n",
    "# Epsilon-Greedy is a policy that handles the exploration/[exploitation trade-off](https://medium.com/@thomassimonini/an-introduction-to-deep-reinforcement-learning-17a565999c0c?source=friends_link&sk=1b1121ae5d9814a09ca38b47abc7dc61).\n",
    "# Epsilon Greedy:\n",
    "# - We do **exploitation** (aka our agent selects the action with the highest state-action pair value) with a prob of *1 - ɛ* : \n",
    "# - We do **exploration** (trying random action) *with probability ɛ*:\n",
    "# As the training goes on, **we progressively reduce the epsilon value** since we will **need less and less exploration and more exploitation**.\n",
    "\n",
    "def epsilon_greedy_policy(q_table, state):\n",
    "    \n",
    "  # if random number > greater than epsilon --> exploitation \n",
    "  # remember epsilon starts at 1 and decals to 0.001 at a rate of 0.01\n",
    "  if(random.uniform(0,1) > epsilon):\n",
    "\n",
    "      # the following line selects the index of the highest point value of the available moves for the state at the index of the state\n",
    "    action = np.argmax(q_table[state])\n",
    "\n",
    "  # else --> exploration (i.e. random choice from action_space)\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "  \n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the Q learning algorithm:\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        action = epsilon_greedy_policy(q_table, state) # returns action (num 0 - 5)\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action) \n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        q_table[state][action] = q_table[state][action] + learning_rate * (reward + gamma * \n",
    "                                    np.max(q_table[new_state]) - q_table[state][action])      \n",
    "        # If done : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 7.61\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "rewards = []\n",
    "\n",
    "frames = []\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    # print(\"****************************************************\")\n",
    "    # print(\"EPISODE \", episode)\n",
    "    for step in range(max_steps):\n",
    "        # env.render()     \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(q_table[state][:])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        total_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            #print (\"Score\", total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE ABOVE WAS USING https://colab.research.google.com/gist/simoninithomas/466c81aa1c2a07dd14793240c6d033c5/q-learning-with-taxi-v3.ipynb#scrollTo=WlJYOh0yBHZO\n",
    "# THE BELOW IS ANOTHER VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.reset()\n",
    "\n",
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the training algorithm that will update this Q-table as the agent explores the environment over thousands of episodes.\n",
    "\n",
    "In the first part of `while not done`, we decide whether to pick a random action or to exploit the already computed Q-values. This is done simply by using the `epsilon` value and comparing it to the `random.uniform(0, 1)` function, which returns an arbitrary number between 0 and 1.\n",
    "\n",
    "We execute the chosen action in the environment to obtain the `next_state` and the `reward` from performing the action. After that, we calculate the maximum Q-value for the actions corresponding to the `next_state`, and with that, we can easily update our Q-value to the `new_q_value`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.40678459,  -2.27325184,  -2.41361289,  -2.36065942,\n",
       "       -10.99713999, -10.85964757])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that the Q-table has been established over 100,000 episodes, let's see what the Q-values are at our illustration's state:\n",
    "q_table[328]\n",
    "\n",
    "# array([ -2.40678459,  -2.27325184,  -2.41361289,  -2.36065942, -10.99713999, -10.85964757])\n",
    "# The max Q-value is \"north\" (-1.971), so it looks like Q-learning has effectively learned the best action to take in our illustration's state!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.75\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "#  Evaluating the agent\n",
    "# We don't need to explore actions any further, so now the next action is always selected using the best Q-value:\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the evaluation, the agent's performance improved significantly and it incurred no penalties, which means it performed the correct pickup/dropoff actions with 100 different passengers.\n",
    "\n",
    "Hyperparameters and optimizations\n",
    "The values of `alpha`, `gamma`, and `epsilon` were mostly based on intuition and some \"hit and trial\", but there are better ways to come up with good values.\n",
    "\n",
    "Ideally, all three should decrease over time because as the agent continues to learn, it actually builds up more resilient priors;\n",
    "\n",
    "ALPHA : (the learning rate) should decrease as you continue to gain a larger and larger knowledge base.\n",
    "GAMMA : as you get closer and closer to the deadline, your preference for near-term reward should increase, as you won't be around long enough to get the long-term reward, which means your gamma should decrease.\n",
    "EPSILON : as we develop our strategy, we have less need of exploration and more exploitation to get more utility from our policy, so as trials increase, epsilon should decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
