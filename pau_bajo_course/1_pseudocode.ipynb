{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train(n_episodes: int):\n",
    "    \"\"\"\n",
    "    Pseudo-code of a Reinforcement Learning agent training loop\n",
    "    \"\"\"\n",
    "\n",
    "    # python object that wraps all environment logic. Typically you will\n",
    "    # be using OpenAI gym here.\n",
    "    env = load_env()\n",
    "\n",
    "    # python object that wraps all agent policy (or value function)\n",
    "    # parameters, and action generation methods.\n",
    "    agent = get_rl_agent()\n",
    "\n",
    "    for episode in range(0, n_episodes):\n",
    "\n",
    "        # random start of the environmnet\n",
    "        state = env.reset()\n",
    "\n",
    "        # epsilon is parameter that controls the exploitation-exploration trade-off.\n",
    "        # it is good practice to set a decaying value for epsilon\n",
    "        epsilon = get_epsilon(episode)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                # Explore action space\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Exploit learned values (or policy)\n",
    "                action = agent.get_best_action(state)\n",
    "\n",
    "            # environment transitions to next state and maybe rewards the agent.\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # adjust agent parameters. We will see how later in the course.\n",
    "            agent.update_parameters(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is epsilon? \n",
    "\n",
    "Epsilon is a key parameter to ensure our agent explores the environment enough, before drawing definite conclusions on what is the best action to take in each state.\n",
    "\n",
    "It is a value between 0 and 1, and it represents the probability the agent chooses a random action instead of what she thinks is the best one.\n",
    "This tradeoff between exploring new strategies vs sticking to already known ones is called the exploration-exploitation problem. This is a key ingredient in RL problems and something that distinguishes RL problems from supervised machine learning.\n",
    "\n",
    "Technically speaking, we want the agent to find the global optimum, not a local one.\n",
    "\n",
    "It is good practice to start your training with a large value (e.g. 50%) and progressively decrease after each episode. This way the agent explores a lot at the beginning, and less as she perfects her strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
