{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter search for Deep Q Learning\n",
    "\n",
    "Deep RL is hard, because (among other things) it's very sensitivity to the hyper-parameters.\n",
    "\n",
    "We tune the hyper-parmeters following a trial&error approach:\n",
    "\n",
    "![](hparams_search_diagram.svg)\n",
    "\n",
    "However, Hyper-parameter spaces in deep RL problems are HUGE. A brute-force solution that would try all possible combinations of hyper-parameters is not feasible. We need something smarter than that...\n",
    "\n",
    "And this is when **Bayesian search** methods enther into the picture.\n",
    "\n",
    "In a nutshell, Bayesian search methods use *past searches* to inform promising avenues.\n",
    "\n",
    "[Optuna](https://optuna.readthedocs.io/en/stable/index.html) is a Python open-source library that implements Bayesian search methods\n",
    "\n",
    "<img src=\"https://github.com/Paulescu/hands-on-rl/blob/main/03_cart_pole/images/optuna.png?raw=True\" width=\"400\"/>\n",
    "\n",
    "Hyper-paramater search a piece of cake üç∞if you use Optuna.\n",
    "\n",
    "### <ins>Paramaters vs Hyperparameters</ins>\n",
    "\n",
    "**Parameters** are the numbers you find AFTER training your model. For example, the parameters of the neural network that maps states to the optimal q-values.\n",
    "\n",
    "The **hyperparameters**, on the other hand, are the numbers you need to set BEFORE you train the model. You cannot find them through training, but you need to correctly guess them beforehand. Once you set them, you train the model and find the remaining parameters.\n",
    "\n",
    "Hyperparameters exist all around Machine Learning. For example, in supervised machine learning problems (like the one we solved in part 5), you need to set the learning rate. Too low of a number and the model will get stuck in local minima. Too large of a number and the model will oscillate too much and never converge to the optimal parameters.\n",
    "\n",
    "In Deep Reinforcement Learning things get even more challenging.\n",
    "\n",
    "**<ins>Why?</ins>**\n",
    "\n",
    "Because\n",
    "\n",
    " - Deep RL algorithms have more hyperparameters than supervised machine learning models.\n",
    " - And, even more importantly, hyperparameters in Deep RL have a huge impact on the final training outcome. In other words, deep RL algorithms are very sensitive to the hyperparameters you set beforehand. The more complex the environment, the more critical hyperparameters are.\n",
    "\n",
    "In part 5 we saw how two sets of hyperparameters, combined with the same parameterization of the q-value network, lead to two very different agents, with very different performances. One of the agents was okayish (with an average reward of around 180), while the other was a perfect solution (average reward 500).\n",
    "\n",
    "<ins>How do we find good hyperparameters?</ins>\n",
    "\n",
    "Essentially, these are the 4 steps:\n",
    "\n",
    " - We choose a set of hyperparameters,\n",
    " - Train the agent,\n",
    " - Evaluate the agent.\n",
    "\n",
    " - If we are happy with the result, we are done. Otherwise, we choose a new set of hyperparameters and repeat the whole process.\n",
    "\n",
    " <ins>GRID SEARCH</ins>\n",
    "\n",
    " If the number of hyperparameters is low (e.g. 2‚Äì3) we can try all possible combinations and select the one that works best. This method is called `grid search`, and it works well for many supervised ML problems.\n",
    "\n",
    "For example, if our algorithm has only 2 hyperparameters, each of them taking 1 out of 5 possible values, we end up with 5 x 5 = 25 combinations. We can train the agent 25 times, using each hyperparameter combination, and find the best ones.\n",
    "\n",
    "![](grid_search.jpeg)\n",
    "\n",
    "<ins>Grid search problems with DL</ins>\n",
    "\n",
    "In Deep RL problems, on the other hand, there are many more hyperparameters, e.g 10‚Äì20. And each hyperparameter can take many possible values.\n",
    "\n",
    "This creates a massive grid in which we need to search, where the number of combinations grows exponentially with respect to the number of hyperparameters and the number of values each hyperparameter can take. In this case, Grid Search is no longer a feasible way to search.\n",
    "\n",
    "For example, if we have 10 hyperparameters, and each can take 1 out of 10 possible values, the grid size is 10,000,000,000 (a 1 followed by 10 zeros). If our training loop takes 10 minutes to complete (which is a very optimistic estimate), it would take us\n",
    "\n",
    "10 minutes x 10,000,000,000 = 190,258 years üòµ‚Äçüí´\n",
    "\n",
    "Which in plain words means‚Ä¶ impossible.\n",
    "\n",
    "You could parallelize the search and decrease this number by a few orders of magnitude. For example, if you had a large cluster of computers that could spin up to 100,000 parallel processes, it would take you around 2 years‚Ä¶\n",
    "\n",
    "Still, this is a very inefficient way to solve the problem, don‚Äôt you think?\n",
    "\n",
    "<ins>Random search</ins>\n",
    "\n",
    "An alternative to grid search, often used in supervised ML problems, is random search.\n",
    "\n",
    "The idea is simple: instead of checking each of the N possible hyperparameter combinations (where N is a very large number, e.g. 1,000,000,000) we randomly try a subset of them with size T (where T is much smaller than N, e.g. 100), and train and evaluate the agent T times. From these T trials, we select the combination of hyperparameters that worked best.\n",
    "\n",
    "Random search is a more efficient way to search and works much better than Grid Search, in terms of speed and quality of the solution.\n",
    "\n",
    "However, with random search (as the name suggests) you are essentially spinning a roulette in each iteration to decide what hyperparameter to try next. And this seems a pretty dumb way to search for something, doesn‚Äôt it?\n",
    "\n",
    "There must be a smarter way, right? ü§î\n",
    "\n",
    "**<ins>2. The solution: Bayesian search</ins>**\n",
    "\n",
    "To search well (whatever you are searching for in life), it is generally a good idea to remember what you tried in the past, and use that information to decide what is best to try next.\n",
    "\n",
    "This is exactly what `Bayesian search` methods do.\n",
    "\n",
    "Bayesian search methods keep track of past iteration results to decide what are the most promising regions in the hyperparameter space to try next.\n",
    "\n",
    "With Bayesian search, you explore the space with the help of a surrogate model, that gives you an estimate of how good each hyperparameter combination is. As you run more iterations, the algorithm updates the surrogate model, and these estimates get better and better.\n",
    "\n",
    "Eventually, your surrogate model is good enough to point your search towards good hyperparameters. And voila, this is how you get them!\n",
    "\n",
    "Bayesian search methods are superior to random search, and a perfect choice to use in Deep RL.\n",
    "\n",
    "Different Bayesian search algorithms differ in how they build the surrogate model. One of the most popular methods is the Tree-structured Parzen Estimator (aka TPE). This is the method we will use today.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/hyperparameters-in-deep-rl-f8a9cf264cd6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: unknown file attribute: i\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
